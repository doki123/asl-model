A neural network used to identify American Sign Language letters and numbers

**USING THE KAGGLE MODEL:**

1. Create a new notebook in Google Colab, and mount the Google Drive.
2. Download the dataset from https://www.kaggle.com/code/gpiosenka/callback-continue-or-halt-training-f1-96/notebook into Google Drive. 
	- Originally, the entire dataset was downloaded directly into the user's computer system. However, due to the size of the complete dataset, many classes did not end up downloaded. Next, subfolders were downloaded individually into Google Drive to ensure all images get transferred; while downloading the different classes, however, the disparity of image counts between classes was becoming increasingly large (eg the sign for _0_ had barely 100 images, while the sign for _a_ had over 1000). In order to both reduce the difference and the download time for the dataset, the subfolders were manually cancelled once 500-1000 images were downloaded. 
3. At line 35 in kaggle_model.py, line _sdir = 'drive/MyDrive/processed_combine_asl_dataset'_, set the directory to the location of the dataset in the Drive.   
4. At line 129, line _max_samples = 50_, the original source code used 250 samples. However, at 50 samples, the accuracy of the model was still in the 90s while reducing the training time significantly. Additionally, on the next line, while _min_samples = 0_ does work well for the model without incident, it can still be set higher should the user wish so.
5. At line 217, line _working_dir = 'drive/MyDrive'_, the directory must once again be set to the Google Drive so that augmented images can be stored within. 
6. From lines 311-371, a function is created wherein after a certain amount of epochs, the model training will stop and ask for input from the user for a) whether the user wants to continue training and b), should they want to continue training, how many epochs the user wants trained until the computer asks for input again. 
	- In the next section, at lines 374 and 375, variable _epochs_ is the maximum amount of epochs that the machine will potentiall run in total. Variable _ask_epoch_ is how many epochs the machine will run before asking for input. If you want to avoid repeatedly checking your computer and giving inputs for your desired epoch length, set _ask_epochs_ to the full epochs you want ran. This way, you will only have to give input once to end the training. 
7. From lines 439-479, a function _def predictor()_ is created specifically to predict on the testing dataset and create a confusion matrix that displays the accuracy of the model. However, a simple model.predict() can be used to predict on single images, which can be seen in the companion file. 
8. At line 486, the model is saved in a file named after the accuracy of the model; however, it can also be more simply named to whatever you want by deleting the above four lines and simply writing _save_id = '[INSERT_TITLE].h5_.


**USING THE WEBCAM AND PREDICTIONS**

1. The first three lines of webcam.py create the folder that stores live-captured images (_captured_images_), the folder that stores the processed iamges (_annotated_images_), and installs mediapipe. Once done, however, it can be commented out in future runs. 
2. In line 20, _last_photo = sorted(os.listdir('captured_images'), reverse=True)_, this variable is used both to check for the existence of .ipynb_checkpoints and to use the number of the last image captured, if images already exist in the folder. However, .ipynb_checkpoints will appear at random, and therefore the next line (_last_photo.remove('.ipynb_checkpoints')_) may or may not be relevant. In order to check, print out _last_photo_ before commenting out line 21. The same thing applies for lines 90 and 169. 
3. Variable _photo_num_ is used to number the images captured, starting from appending 0 to the name of the first image saved into _captured_images_, and increasing by 1 to every subsequent image captured. 
4. Lines 35-65 create the javascript code necessary to create a frame that displays footage captured from the user's computer. 
5. Line 105 _IMAGE_FILES = 'captured_images/'_ sets the directory to the folder full of captured images to be processed on; change depending on the naming structure you use. 
6. Line 148 'cv2.imwrite('annotated_images/annotated_image_' + names[count] + '.png', cv2.flip(annotated_image, 1))', _annotated_images_ is the folder to save processed images into; change depending on the naming structure you use. 
7. Line 161 _reconstructed_model = keras.models.load_model("drive/MyDrive/asl2_89.72.h5")_, this loads the model created previously; change the name depending on what you saved the model as. 
8. The reason that the predictions must be converted first is because of the nature of the image's shape--the original model predicts on images using dimensions of (None, 200, 200, 3). However, because the dimensions of these images are 400x400, (400x400)/(200x200x3) has an extra 4, creating an image with the shape (4, 200, 200, 3) instead. Because of this, the array that model.predict() produces is a tuple containing four arrays of 36, rather than just being one array of 36. Therefore, np.argmax(predictions) has the possibiliy of resulting in a number greater than 35; to remedy this, the result is subtracted by 36 repeatedly until it comes in range of 0-35. This, then, can be plugged in as an index to _classlist_, which is a list containing the numbers 0-9 and 'a' through 'z'. The final result, then, will be the predicted name of the sign given.

**GENERAL NOTES**

This application is a neural network created in Google Colab, based on source code and trained on a dataset sourced from Kaggle. Working in tandem to this, images from a webcam are captured and processed using the mediapipe library. This converted data can then be fed into a saved version of the model in order to return its predicted classification. Mediapipe can superimpose a hand’s major points–specifically, the joints of fingers and the palm–onto an image and return the coordinates of each landmark. The usage of this library can increase the accuracy of the predictions by cutting out the background noise of colors and obscuring objects. 

Additionally, though Google Colab was the chosen platform for this project specifically due to its non-local location, this caused difficulty accessing the computer’s local webcam to capture images. This created a secondary issue wherein mediapipe could not be directly applied onto live footage, in contrast to a prototype model that captured and converted images into landmarks in real time. However, a workaround was ultimately implemented where separate images are captured and then processed afterward. 

There were difficulties feeding in self-captured images into the model itself during the training process, which is being worked on by pre-training the model, saving it, and using self-captured images only for predictions.

While testing the model, it was observed that predictions were consistently skewed to the ‘f’ or ‘g’ sign, despite the self-calculated accuracy for the model being in the high 90s. While combing through the model and comparing the outputs to the original Kaggle code, it was noticed that there were far fewer files in the dataset as compared to the original code; furthermore, there were only six classes in total noted in the dataframe, while there should have been 36. 

Further examination led to the conclusion that in the process of downloading the images to a google drive for use, many of the subfolders had been dropped due to the size of the dataset. To rectify this, the dataset was redownloaded subfolder by subfolder, but that too proved difficult due to the long wait-time of downloads, should the dataset be downloaded in its entirety. However, upon closer inspection, it was noticed that some classes of signs only had 60 or so images, while others had images into the thousands. In order to both cut down on the wait-time and to equalize the large disparity in data, the downloads were manually halted once around five hundred to one thousand images had been downloaded into each folder. 

Features I would like to add is a full, working HTML website from which to access this model and predict signs. I would also like to test the optimization for the maximum amount of images per class, as well as supplement the dataset with more images to equalize the data quantities in each class.  

The amount of epochs that are run can depend on the user-–acceptable accuracy percentages were achieved after the first five epochs alone. Even with a reduced training dataset using 20% of the original code's amount of samples, accuracy values in the 90 percent ranges were reached. Finally, in order to access the saved model after the run time of the notebook is deleted, save the model into the mounted google drive. 

In order to run the webcam portion of the code, the user will first need to create a folder to save captured images in. In google colab, a new folder can be automatically created with the !mkdir new_folder command, or the user can alternatively store the images inside a mounted google drive. The user may also need to run a !pip install mediapipe command should the mediapipe sections of code not function correctly. Save the captured images within the designated folder and run the mediapipe section of code in order to capture the hand landmarks. 

The dataset and source code for the model comes from this Kaggle post: https://www.kaggle.com/code/gpiosenka/callback-continue-or-halt-training-f1-96/notebook. The source code for the webcam comes from this notebook: https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=SucxddsPhOmj.
